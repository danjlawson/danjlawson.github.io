<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>BayesianModelling.md</title>
  <link rel="stylesheet" href="http://danjlawson.github.io/style.css" />
	<style>
	body {
		background-color: linen;
	}
</style>
</head>

<body class="stackedit">
  <div class="stackedit__html"><h1 id="bayesian-modelling-of-epidemic-processes">Bayesian Modelling of Epidemic Processes</h1>
<p>By <a href="http://www.maths.bristol.ac.uk/~madjl/">Daniel Lawson</a> from the University of Bristol <a href="http://www.bristol.ac.uk/maths/people/group/maths-statistics/4924">School of Statistical Science</a>.</p>
<h2 id="bayesian-modelling">Bayesian Modelling</h2>
<p><strong>Statistical inference</strong>: Bayesian modelling is a statistical inference procedure. Inference means learning from data. You have all done Likelihood-based inference, in which we know the probability of the data given the parameters, and invert to find the parameters for which this is maximised.</p>
<p>Bayesian inference uses Bayes Theorem,<br>
<span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>θ</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>x</mi><mi mathvariant="normal">∣</mi><mi>θ</mi><mo stretchy="false">)</mo><mi>P</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">
P(\theta | x) = \frac{P(x | \theta) P(\theta)}{P(x)},
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right: 0.02778em;">θ</span><span class="mord">∣</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 2.363em; vertical-align: -0.936em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.427em;"><span class="" style="top: -2.314em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.677em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right: 0.02778em;">θ</span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right: 0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right: 0.02778em;">θ</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.936em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mpunct">,</span></span></span></span></span></span><br>
to write a <strong>Posterior</strong> probability for the parameters given the data. This uses the <strong>Likelihood</strong> <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>x</mi><mi mathvariant="normal">∣</mi><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(x | \theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right: 0.02778em;">θ</span><span class="mclose">)</span></span></span></span></span> and the <strong>Prior</strong> <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(\theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right: 0.02778em;">θ</span><span class="mclose">)</span></span></span></span></span>.</p>
<p>It is hard because we often cannot compute the normalising constant <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mo>∫</mo><mi>P</mi><mo stretchy="false">(</mo><mi>x</mi><mi mathvariant="normal">∣</mi><mi>θ</mi><mo stretchy="false">)</mo><mi>P</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mi>d</mi><mi>θ</mi></mrow><annotation encoding="application/x-tex">P(x) = \int P(x | \theta) P(\theta) d\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1.11112em; vertical-align: -0.30612em;"></span><span class="mop op-symbol small-op" style="margin-right: 0.19445em; position: relative; top: -0.00056em;">∫</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault" style="margin-right: 0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right: 0.02778em;">θ</span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right: 0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right: 0.02778em;">θ</span><span class="mclose">)</span><span class="mord mathdefault">d</span><span class="mord mathdefault" style="margin-right: 0.02778em;">θ</span></span></span></span></span>.</p>
<h3 id="what-bayesian-inference-means">What Bayesian Inference means</h3>
<p>If we provide a Prior that represents our true subjective beliefs, and a Model that contains all the possibilities that we believe could be true, then the Posterior is the correct probability of the parameters given the data. It is a consistent update rule, meaning that we will always get this answer, whatever order we see the data. It is “the right thing to do”.</p>
<p>Incorporating prior knowledge is often vital. See for example <a href="https://xkcd.com/1132/">XKCD: Did the sun just explode?</a>.</p>
<p>The two caveats are key, however. We often do not know how to completely specify our subjective beliefs over a complicated parameter space. We also rarely believe that we are entertaining all possible models that could be true.</p>
<ul>
<li><strong>Model misspecification</strong>: If we do not include the true model in <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.02778em;">θ</span></span></span></span></span>, then we are searching instead for the “least bad solution in the parameter space”. This can have weird properties, because we don’t just get a point estimate - we are calculating the posterior which is a set of estimates (whose expectation is closest to the truth). See e.g. Figure 3 of Grunwald and van Ommen 2017 <a href="https://projecteuclid.org/download/pdfview_1/euclid.ba/1510974325">Inconsistency of Bayesian Inference for Misspecified Linear Models…</a></li>
<li><strong>Prior misspecification</strong>: We often use convenience priors for computational reasons (see below). We almost always specify our priors on each parameter independently or at best pairwise, as quantifying high-dimensional spaces is hard. This is a real problem for Process Modelling such as for Epidemics, where parameters are correlated because they are chosen to “mean something”.</li>
</ul>
<p>The end result of these issues is that Bayesian Inference is often best considered to be a computational procedure for arriving at parameter estimates, rather than about beliefs.</p>
<h3 id="how-to-do-bayesian-inference">How to do Bayesian Inference</h3>
<p>Some key concepts:</p>
<ul>
<li><strong>Numerical Integration</strong> is key to Bayesian Inference.
<ul>
<li>The simplest approach is numerical integration or Monte-Carlo approaches, but these are not very efficient.</li>
<li><strong>MCMC</strong>: Instead we typically use <a href="https://www.cs.ubc.ca/~arnaud/andrieu_defreitas_doucet_jordan_intromontecarlomachinelearning.pdf">Markov-Chain Monte-Carlo (MCMC)</a> (or see <a href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo">Wikipedia</a>) because it behaves better with dimension than regular numerical integration.</li>
</ul>
</li>
<li><strong>Intractable Likelihoods (ABC)</strong>: To use MCMC requires being able to compute the Likelihood. Sometimes this is also intractable, e.g. if it involves many latent parameters, stochastic integration, etc. (NB <a href="https://www.stats.ox.ac.uk/~doucet/andrieu_doucet_holenstein_PMCMC.pdf">Christophe Andrieu’s particle MCMC</a> paper made a class of such models tractable, it may be helpful here. But there are always intractable models).
<ul>
<li>If the likelihood is intractable, we can instead simulate from the prior and evaluate model fit. This is called <a href="https://www.annualreviews.org/doi/abs/10.1146/annurev-statistics-030718-105212">Approximate Bayesian Computation</a> and was (co-)invented by Mark Beaumont at Bristol.</li>
<li>Doing this approximates the likelihood using the model fit. This is really difficult to get right and somewhere that Machine Learning is very important.</li>
<li>The goal here is essentially to learn a set of summary statistics whose distance from the “true” parameter values captures the likelihood.</li>
</ul>
</li>
<li><strong>Identifiability</strong> is a really important idea. It says that the parameters can be inferred uniquely from the data, if we had enough of it. Most of the models we are interested in are not identifiable, or if they are, they are “weakly identifiable” meaning that we can’t practically expect to get the value. It affects parameter inference because if the parameters are unidentifiable, the posterior distribution doesn’t converge to a point, and is therefore a complex object which is really hard to sample from.</li>
<li><strong>Model choice</strong> is also really important. Above I wrote we must "include the true model in <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.02778em;">θ</span></span></span></span></span>". Its long debated whether choosing between two models <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi><mo>=</mo><mo stretchy="false">{</mo><msub><mi mathvariant="script">M</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi mathvariant="script">M</mi><mn>2</mn></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\theta=\{\mathcal{M}_1,\mathcal{M}_2\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.02778em;">θ</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">{</span><span class="mord"><span class="mord"><span class="mord mathcal">M</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord"><span class="mord mathcal">M</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">}</span></span></span></span></span> is the same as parameter estimation. The short answer is “no”, model choice (in which parameters mean different things in different models) is really hard.</li>
<li>Advanced methods include:
<ul>
<li><strong>Variational inference</strong>: abandoning the idea that a posterior is about beliefs, and the idea that we want to report the posterior. Instead of sampling from it, we can approximate it using the <em>nearest parametric fit</em>, and optimise the fit of this <em>whole distribution</em> to the posterior. This changes the problem from integration/sampling to optimisation, which is typically much easier.</li>
<li><strong>Parallelisation</strong> by various divide and conquer approaches including Expectation Propagation (EP)</li>
</ul>
</li>
</ul>
<h2 id="epidemic-processes">Epidemic Processes</h2>
<p>Epidemic processes are dynamical systems in which the state  <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϕ</mi><mo stretchy="false">(</mo><mi>t</mi><mo>+</mo><mi>δ</mi><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\phi(t+\delta t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault">ϕ</span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.03785em;">δ</span><span class="mord mathdefault">t</span><span class="mclose">)</span></span></span></span></span> at time <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi><mo>+</mo><mi>δ</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">t+\delta t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.69841em; vertical-align: -0.08333em;"></span><span class="mord mathdefault">t</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.03785em;">δ</span><span class="mord mathdefault">t</span></span></span></span></span> depends on <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϕ</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\phi(t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault">ϕ</span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mclose">)</span></span></span></span></span>, the state at time <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.61508em; vertical-align: 0em;"></span><span class="mord mathdefault">t</span></span></span></span></span>. Some statistical models, that is, models for which we can write an explicit likelihood, have this property. These include renewal processes, ARIMA models, etc. However, most dynamical systems and especially those with interesting intrinsic dynamics have no explicit form.</p>
<p>The hierarchy of models might be described as:</p>
<ul>
<li><strong>ODEs</strong> (Ordinary Differential Equations): a set of variables co-evolve according to some ODE. This makes the output deterministic given the input, and therefore a Likelihood can be written as an “error model”, that is, how far is the data from the prediction?</li>
<li><strong>PDEs</strong> (Partial Differential Equations): a set of variables in space co-evolve according to a PDE. These are also deterministic but numerical integration is much harder, and the initial parameter space is technically infinite (the boundary conditions are a function, not a set of parameters).</li>
<li><strong>SDEs</strong> (Stochastic Differential Equations): a set of variables co-evolve according to some equation with a deterministic plus a noise term. Some SDEs are nicely behaved, e.g. can be described by Gaussian noise and solved with a Kalman Filter or <a href="https://www.stats.ox.ac.uk/~doucet/andrieu_doucet_holenstein_PMCMC.pdf">particle MCMC</a> but many are not. Numerical integration is often hard and the output is probabilistic.</li>
<li><strong>Compartmental models</strong>: Generalising the above, space, age categories, households, etc can be split up into multiple “bins” for which a ODE/SDE can be written.</li>
<li><strong>IBMs</strong> (Individual Based Models): the highest fidelity representation of reality is a detailed model describing every detail we think might be important. These are very hard to infer because they are costly to simulate, and there are so many parameters that have very similar effects on how the model behaves.</li>
</ul>
<p>In all cases, these dynamical systems may have interesting intrinsic structure. For example, there may be a “phase transition” where an epidemic changes from being periodic, to happening only once, or happening not at all.</p>
<h3 id="open-research-questions-for-the-ug-projects">Open research questions for the UG projects</h3>
<p>There are open directions that can be explored.</p>
<h4 id="model-mis-specification">Model mis-specification</h4>
<ol>
<li>If there was a true, complex model (say, a compartmental one similar to our paper, <a href="https://www.medrxiv.org/content/10.1101/2020.06.10.20084715v1">Booton et al 2020</a>) and we performed inference with a simpler model that lacked the fine-grained detail, what would be lost? Under which circumstances? Are the parameters for COVID-19 well-behaved?</li>
<li>What if we replace the above compartmental model by an SDE, which we infer with an ODE? ODEs lead to a “likelihood” for the residuals, which is normally simple &amp; independent (normal IID, negative binomial, etc). Can we change the likelihood to make it work better? This represents the “cost” of using a solvable dynamical model that can be handled in <a href="https://mc-stan.org">STAN</a> when the truth is something more complicated.</li>
<li>If there was a true epidemic model, but we used a statistical model chosen for tractability, what is lost? What questions provide “honest” (e.g. unbiased) answers, and which could result in dangerous inference due to model mis-specification?</li>
</ol>
<h4 id="approximate-bayesian-inference">Approximate Bayesian Inference</h4>
<ol start="4">
<li>How well do current state-of-the-art approaches to <strong>summary statistic selection</strong> perform on epidemic models? Do the accessible machine learning approaches work better? Under which circumstances, e.g. high number of parameters, when the dynamical system is near a phase transition, etc.</li>
<li>What state-of-the-art Machine learning approaches could be incorporated into these models?</li>
<li>What is the advantage of using an SDE vs ODE? Given that we cannot do STAN inference on an SDE.</li>
</ol>
<h4 id="conceptual-questions">Conceptual questions</h4>
<ol start="7">
<li>How does Bayesian Inference resemble maximum likelihood inference for epidemic models? How much information is coming from the prior, vs the data?</li>
<li>What additional data we need to collect in order to make complex (high dimensional) parameter estimation procedures work?</li>
<li>How does the computational performance of MCMC behave as the complexity of the model grows? Are there practical limits to how complex the model can be?</li>
<li>What is lost when using alternative inference approaches such as variational methods? Are any of them both fast enough and accurate enough to be useful?</li>
</ol>
<h2 id="covid-19-and-directly-relevant-papers">COVID-19 and directly relevant Papers</h2>
<h3 id="approximate-bayesian-computation">Approximate Bayesian Computation</h3>
<p>Chandra 2020 <a href="https://www.medrxiv.org/content/10.1101/2020.03.29.20046862v1">“Stochastic Compartmental Modelling of SARS-CoV-2 with Approximate Bayesian Computation”</a>.</p>
<h3 id="full-bayesian-modelling-using-odes">Full Bayesian Modelling using ODEs</h3>
<p>This is essentially using the toolkit STAN, following the recipe outlined by Grinsztajn et al 2020 in <a href="https://arxiv.org/abs/2006.02985">Bayesian workflow for disease transmission modeling in Stan</a>.</p>
<p>There are few application papers to be found, however.</p>
<p>There is the <a href="https://www.medrxiv.org/content/10.1101/2020.04.28.20083873v1">South Africa paper</a> that fails to cite STAN.</p>
<p>A different sampler is used in <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7104073/">Why is it difficult to accurately predict the COVID-19 epidemic?</a> Is it as good?</p>
<h3 id="full-bayesian-modelling-using-approximations">Full Bayesian Modelling using approximations</h3>
<p>Most approaches you find will use this strategy; they change the model and predict a particular aspect of it.</p>
<h3 id="sampling-approaches-with-a-bayesian-interpretation">Sampling approaches with a Bayesian Interpretation</h3>
<p>Our paper, Booton et al 2020 <a href="https://www.medrxiv.org/content/10.1101/2020.06.10.20084715v1">Estimating the COVID-19 epidemic trajectory and hospital capacity requirements in South West England: a mathematical modelling framework</a> uses a more complex model and samples parameter space.</p>
<p>A Machine Learning approach for <a href="https://www.sciencedirect.com/science/article/pii/S0960077920302538">Short-term forecasting COVID-19 cumulative confirmed cases: Perspectives for Brazil</a> (Dal Molin Ribeiro et al 2020).</p>
<h2 id="background-reading">Background reading</h2>
<h3 id="approximate-bayesian-computation-1">Approximate Bayesian Computation</h3>
<ul>
<li>Start with <a href="https://www.annualreviews.org/doi/abs/10.1146/annurev-statistics-030718-105212">Mark Beaumont’s 2019 Review on ABC</a>.</li>
<li>There is a <a href="https://www.facebook.com/groups/921246634574876">Facebook working group</a> managed by Dennis Prangle, a key ABC researcher.</li>
</ul>
<p>Fraser 2020 preprint on <a href="https://arxiv.org/abs/2006.14126?fbclid=IwAR0H_CmhzV5tVfZf-r_hdrkA_TxTx8Ng59naYjA5yRA2cWSoRD-05PpHeHI">ABC that is robust to model mis-specification</a></p>
<p>Not all ABC needs sampling. Variational approaches are interesting. This from Dennis Prangles group: <a href="https://arxiv.org/abs/1802.03335">Black-box Variational Inference for Stochastic Differential Equations</a></p>
<h4 id="toolkits-for-abc">Toolkits for ABC:</h4>
<ul>
<li>Simulation based inference <a href="http://www.mackelab.org/sbi/">SBI python package</a> with the reference <a href="https://arxiv.org/abs/1911.01429">The frontier of simulation-based inference</a>. This focusses on the Machine Learning aspects.</li>
<li>Jabot et al 2013 <a href="https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12050">EasyABC R package</a> provides several ABC sampling algorithms.</li>
<li>Michael Blum’s <a href="https://cran.r-project.org/web/packages/abc/vignettes/abcvignette.pdf">abc R package</a></li>
<li>There are more packages, please add to this list!</li>
</ul>
<h3 id="machine-learning-and-abc">Machine Learning and ABC</h3>
<ul>
<li>Can the approach by Mondal et al  2019 <a href="https://www.nature.com/articles/s41467-018-08089-7/">Approximate Bayesian computation with deep learning supports a third archaic introgression in Asia and Oceania</a> by adapted for process models?</li>
<li>Similarly Åkesson et al’s 2020 preprint <a href="https://arxiv.org/abs/2001.11760">Convolutional Neural Networks as Summary Statistics for Approximate Bayesian Computation</a></li>
<li>And an important NeurIPs paper from Yun Song’s group: <a href="https://papers.nips.cc/paper/8078-a-likelihood-free-inference-framework-for-population-genetic-data-using-exchangeable-neural-networks.pdf">A Likelihood-Free Inference Framework for Population Genetic Data using Exchangeable Neural Networks</a></li>
</ul>
<h2 id="bayesian-approaches">Bayesian approaches</h2>
<ul>
<li><a href="https://pdfs.semanticscholar.org/9a12/27f2fc93d22e13c649e97cfffafc97f3127e.pdf">Bayesian Choice Textbook by Christian Robert, 2007</a></li>
<li>John Paisley’s course notes on <a href="http://www.columbia.edu/~jwp2128/Teaching/E6720/BayesianModelsMachineLearning2016.pdf">Bayesian Modelling for Machine Learning</a></li>
<li>Larry Wasserman’s course notes on <a href="http://www.stat.cmu.edu/~larry/=stat705/Lecture14.pdf">Why Bayes Theorem is not Bayesian Inference</a> (Thanks to Larry for the XKCD comic)</li>
</ul>
<h3 id="notes">Notes</h3>
<blockquote>
<p>Written with <a href="https://stackedit.io/">StackEdit</a>.</p>
</blockquote>
</div>
</body>

</html>
